训练集样本数: 8528
测试集样本数: 2133
Traceback (most recent call last):
  File "/home/wuwen/python_project/nlp2/main.py", line 101, in <module>
    train(args)
  File "/home/wuwen/python_project/nlp2/main.py", line 84, in train
    trained_model = trainer.trainer(device, model, optimizer, criterion, train_dataloader, test_dataloader, bert, args.epochs)
  File "/home/wuwen/python_project/nlp2/trainer.py", line 15, in trainer
    for input_ids, attention_mask, labels in train_dataloader:
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/wuwen/python_project/nlp2/model_data.py", line 72, in <lambda>
    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, tokenizer))
  File "/home/wuwen/python_project/nlp2/model_data.py", line 26, in collate_fn
    encodings = tokenizer(list(texts),
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
    return self.batch_encode_plus(
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 3311, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 888, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 855, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 698, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py", line 161, in _tokenize
    for token in self.basic_tokenizer.tokenize(
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py", line 348, in tokenize
    text = self._tokenize_chinese_chars(text)
  File "/home/wuwen/miniconda3/envs/cs224n_dfp/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py", line 404, in _tokenize_chinese_chars
    if self._is_chinese_char(cp):
KeyboardInterrupt
